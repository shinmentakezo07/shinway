import { redisClient } from "@llmgateway/cache";
import { logger } from "@llmgateway/logger";

import { estimateTokens } from "./estimate-tokens.js";

import type { Annotation, ImageObject } from "./types.js";
import type { Provider } from "@llmgateway/models";

/**
 * Parses response content and metadata from different providers
 */
export function parseProviderResponse(
	usedProvider: Provider,
	json: any,
	messages: any[] = [],
) {
	let content = null;
	let reasoningContent = null;
	let finishReason = null;
	let promptTokens = null;
	let completionTokens = null;
	let totalTokens = null;
	let reasoningTokens = null;
	let cachedTokens = null;
	let toolResults = null;
	let images: ImageObject[] = [];
	let annotations: Annotation[] = [];
	let webSearchCount = 0;

	switch (usedProvider) {
		case "aws-bedrock": {
			// AWS Bedrock Converse API format
			// Response format: { output: { message: { content: [{text: "..."}], role: "assistant" }}, stopReason: "end_turn", usage: {...} }
			const message = json.output?.message;
			const contentBlocks = message?.content || [];

			// Extract text content from content blocks
			content =
				contentBlocks
					.filter((block: any) => block.text)
					.map((block: any) => block.text)
					.join("") || null;

			// Map Bedrock stop reasons to OpenAI finish reasons
			const stopReason = json.stopReason;
			if (stopReason === "end_turn") {
				finishReason = "stop";
			} else if (stopReason === "max_tokens") {
				finishReason = "length";
			} else if (stopReason === "tool_use") {
				finishReason = "tool_calls";
			} else if (stopReason === "content_filtered") {
				finishReason = "content_filter";
			} else {
				finishReason = "stop"; // default fallback
			}

			// Extract usage tokens (including cached tokens for prompt caching)
			if (json.usage) {
				const inputTokens = json.usage.inputTokens || 0;
				const cacheReadTokens = json.usage.cacheReadInputTokens || 0;
				const cacheWriteTokens = json.usage.cacheWriteInputTokens || 0;

				// Total prompt tokens = regular input + cache read + cache write
				promptTokens = inputTokens + cacheReadTokens + cacheWriteTokens;
				completionTokens = json.usage.outputTokens || null;
				totalTokens = json.usage.totalTokens || null;
				// Cached tokens are the tokens read from cache (discount applies to these)
				cachedTokens = cacheReadTokens || null;
			}

			// Extract tool calls if present
			const toolUseBlocks = contentBlocks.filter((block: any) => block.toolUse);
			if (toolUseBlocks.length > 0) {
				toolResults = toolUseBlocks.map((block: any) => ({
					id: block.toolUse.toolUseId,
					type: "function",
					function: {
						name: block.toolUse.name,
						arguments: JSON.stringify(block.toolUse.input),
					},
				}));
			}

			break;
		}
		case "anthropic": {
			// Extract content and reasoning content from Anthropic response
			const contentBlocks = json.content || [];
			const textBlocks = contentBlocks.filter(
				(block: any) => block.type === "text",
			);
			const thinkingBlocks = contentBlocks.filter(
				(block: any) => block.type === "thinking",
			);

			content = textBlocks.map((block: any) => block.text).join("") || null;
			reasoningContent =
				thinkingBlocks.map((block: any) => block.thinking).join("") || null;

			finishReason = json.stop_reason || null;

			// Extract web search citations from Anthropic response
			// Anthropic returns web_search_tool_result blocks with content that includes source info
			const webSearchBlocks = contentBlocks.filter(
				(block: any) => block.type === "web_search_tool_result",
			);
			if (webSearchBlocks.length > 0) {
				webSearchCount = webSearchBlocks.length;
				// Extract citations from each web search result
				for (const block of webSearchBlocks) {
					if (block.content && Array.isArray(block.content)) {
						for (const item of block.content) {
							if (item.type === "web_search_result") {
								annotations.push({
									type: "url_citation",
									url_citation: {
										url: item.url || "",
										title: item.title,
									},
								});
							}
						}
					}
				}
			}

			// Also check for citations in text blocks (inline citations)
			for (const block of textBlocks) {
				if (block.citations && Array.isArray(block.citations)) {
					for (const citation of block.citations) {
						annotations.push({
							type: "url_citation",
							url_citation: {
								url: citation.url || "",
								title: citation.title,
								start_index: citation.start_char_index,
								end_index: citation.end_char_index,
							},
						});
					}
				}
			}

			// For Anthropic: input_tokens are the non-cached tokens
			// We need to add cache_creation_input_tokens to get total input tokens
			if (json.usage) {
				const inputTokens = json.usage.input_tokens || 0;
				const cacheCreationTokens = json.usage.cache_creation_input_tokens || 0;
				const cacheReadTokens = json.usage.cache_read_input_tokens || 0;

				// Total prompt tokens = non-cached + cache creation + cache read
				promptTokens = inputTokens + cacheCreationTokens + cacheReadTokens;
				completionTokens = json.usage.output_tokens || null;
				reasoningTokens = json.usage.reasoning_output_tokens || null;
				// Cached tokens are the tokens read from cache (discount applies to these)
				cachedTokens = cacheReadTokens || null;
				totalTokens =
					promptTokens && completionTokens
						? promptTokens + completionTokens
						: null;
			}
			// Extract tool calls from Anthropic format
			toolResults =
				json.content
					?.filter((block: any) => block.type === "tool_use")
					?.map((block: any) => ({
						id: block.id,
						type: "function",
						function: {
							name: block.name,
							arguments: JSON.stringify(block.input),
						},
					})) || null;
			if (toolResults && toolResults.length === 0) {
				toolResults = null;
			}
			break;
		}
		case "google-ai-studio":
		case "google-vertex": {
			// Debug logging at the start to capture raw response structure
			// Only log as error if there's no promptFeedback explaining why candidates are missing
			if (
				(!json.candidates || json.candidates.length === 0) &&
				!json.promptFeedback?.blockReason
			) {
				logger.error(
					"[parse-provider-response] Google response missing candidates",
					{
						hasCandidates: !!json.candidates,
						candidatesLength: json.candidates?.length || 0,
						hasPromptFeedback: !!json.promptFeedback,
						promptBlockReason: json.promptFeedback?.blockReason,
						responseKeys: Object.keys(json),
						fullResponse: JSON.stringify(json, null, 2),
					},
				);
			}

			// Extract content and reasoning content from Google response parts
			const parts = json.candidates?.[0]?.content?.parts || [];
			const contentParts = parts.filter((part: any) => !part.thought);
			const reasoningParts = parts.filter((part: any) => part.thought);

			content = contentParts.map((part: any) => part.text).join("") || null;
			reasoningContent =
				reasoningParts.map((part: any) => part.text).join("") || null;

			// Extract images from Google response parts
			const imageParts = parts.filter((part: any) => part.inlineData);
			images = imageParts.map(
				(part: any): ImageObject => ({
					type: "image_url",
					image_url: {
						url: `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`,
					},
				}),
			);

			// Debug logging to identify parsing issues
			if (!content && !reasoningContent && parts.length > 0 && !images.length) {
				logger.warn(
					"[parse-provider-response] Google response has parts but no text extracted",
					{
						json,
					},
				);
			}

			// Extract tool calls from Google format - reuse the same parts array
			// Include thoughtSignature if present (required for Gemini 3 multi-turn conversations)
			toolResults =
				parts
					.filter((part: any) => part.functionCall)
					.map((part: any, index: number) => {
						const toolCall: any = {
							id: `${part.functionCall.name}_${json.candidates?.[0]?.index ?? 0}_${index}`, // Google doesn't provide ID, so generate one
							type: "function",
							function: {
								name: part.functionCall.name,
								arguments: JSON.stringify(part.functionCall.args || {}),
							},
						};
						// Cache thoughtSignature for multi-turn conversations
						// This allows us to retrieve it when the client sends back the conversation history
						if (part.thoughtSignature) {
							toolCall.extra_content = {
								google: {
									thought_signature: part.thoughtSignature,
								},
							};
							// Store in Redis for server-side retrieval since OpenAI SDKs don't preserve extra_content
							redisClient
								.setex(
									`thought_signature:${toolCall.id}`,
									86400, // 1 day expiration
									part.thoughtSignature,
								)
								.catch((err) => {
									logger.error("Failed to cache thought_signature", { err });
								});
						}
						return toolCall;
					}) || null;
			if (toolResults && toolResults.length === 0) {
				toolResults = null;
			}

			// Also check if text parts have thought signatures and add them to content
			// This allows clients to pass them back in multi-turn conversations
			const textPartsWithSignatures = contentParts.filter(
				(part: any) => part.thoughtSignature,
			);
			if (textPartsWithSignatures.length > 0 && content) {
				// Store the thought signature in a way the client can return it
				// We'll need to enhance the response format to include this
			}

			// Check for prompt feedback block reason (when content is blocked before generation)
			const promptBlockReason = json.promptFeedback?.blockReason;
			const googleFinishReason = json.candidates?.[0]?.finishReason;

			// Preserve the original Google finish reason for logging
			// Use promptBlockReason if present, otherwise use googleFinishReason
			if (promptBlockReason) {
				finishReason = promptBlockReason;
			} else if (googleFinishReason) {
				finishReason = googleFinishReason;
			} else {
				finishReason = null;
			}

			// Debug logging for finish reason mapping
			if (!finishReason) {
				logger.error(
					"[parse-provider-response] Google response missing finish reason",
					{
						promptBlockReason,
						googleFinishReason,
						candidateFinishReason: json.candidates?.[0]?.finishReason,
						hasPromptFeedback: !!json.promptFeedback,
						candidateKeys: json.candidates?.[0]
							? Object.keys(json.candidates[0])
							: [],
					},
				);
			}

			// Extract web search citations from Google grounding metadata
			const groundingMetadata = json.candidates?.[0]?.groundingMetadata;
			if (groundingMetadata) {
				webSearchCount = 1; // Google doesn't report individual search counts
				// Extract from groundingChunks (sources)
				if (
					groundingMetadata.groundingChunks &&
					Array.isArray(groundingMetadata.groundingChunks)
				) {
					for (const chunk of groundingMetadata.groundingChunks) {
						if (chunk.web) {
							annotations.push({
								type: "url_citation",
								url_citation: {
									url: chunk.web.uri || "",
									title: chunk.web.title,
								},
							});
						}
					}
				}
				// Also extract from webSearchQueries if available for reference
				if (
					groundingMetadata.webSearchQueries &&
					groundingMetadata.webSearchQueries.length > 0
				) {
					webSearchCount = groundingMetadata.webSearchQueries.length;
				}
			}

			promptTokens = json.usageMetadata?.promptTokenCount || null;
			completionTokens = json.usageMetadata?.candidatesTokenCount || null;
			reasoningTokens = json.usageMetadata?.thoughtsTokenCount || null;
			// Don't use Google's totalTokenCount as it doesn't include reasoning tokens
			totalTokens = null;

			// If candidatesTokenCount is missing, estimate it from the content or set to 0
			if (completionTokens === null) {
				if (content) {
					const estimation = estimateTokens(
						usedProvider,
						[],
						content,
						null,
						null,
					);
					completionTokens = estimation.calculatedCompletionTokens;
				} else {
					// No content means 0 completion tokens (e.g., MAX_TOKENS with only reasoning)
					completionTokens = 0;
				}
			}

			// Calculate totalTokens to include reasoning tokens for Google models
			if (promptTokens !== null) {
				totalTokens =
					promptTokens + (completionTokens || 0) + (reasoningTokens || 0);
			}
			break;
		}
		case "mistral":
		case "novita": {
			content = json.choices?.[0]?.message?.content || null;
			// Extract reasoning content - check both reasoning and reasoning_content fields
			reasoningContent =
				json.choices?.[0]?.message?.reasoning ||
				json.choices?.[0]?.message?.reasoning_content ||
				null;
			finishReason = json.choices?.[0]?.finish_reason || null;
			promptTokens = json.usage?.prompt_tokens || null;
			completionTokens = json.usage?.completion_tokens || null;
			reasoningTokens = json.usage?.reasoning_tokens || null;
			cachedTokens = json.usage?.prompt_tokens_details?.cached_tokens || null;
			totalTokens = json.usage?.total_tokens || null;

			// Handle Mistral/Novita JSON output mode which wraps JSON in markdown code blocks
			if (
				content &&
				typeof content === "string" &&
				content.includes("```json")
			) {
				const jsonMatch = content.match(/```json\s*([\s\S]*?)\s*```/);
				if (jsonMatch && jsonMatch[1]) {
					// Extract and clean the JSON content
					content = jsonMatch[1].trim();
					// Ensure it's valid JSON by parsing and re-stringifying to normalize formatting
					try {
						const parsed = JSON.parse(content);
						content = JSON.stringify(parsed);
					} catch {}
				}
			}

			// Map non-standard finish reasons to OpenAI-compatible values
			if (finishReason === "end_turn") {
				finishReason = "stop";
			} else if (finishReason === "tool_use") {
				finishReason = "tool_calls";
			}

			// Extract tool calls from Mistral/Novita format (same as OpenAI)
			toolResults = json.choices?.[0]?.message?.tool_calls || null;
			break;
		}
		case "alibaba": {
			// Check if this is a DashScope multimodal generation response (image generation)
			// Format: { output: { choices: [{ message: { content: [{ image: "url" }] } }] }, usage: {...} }
			const alibabaChoices = json.output?.choices;
			if (alibabaChoices && Array.isArray(alibabaChoices)) {
				const messageContent = alibabaChoices[0]?.message?.content;
				if (Array.isArray(messageContent)) {
					// Extract images from content array
					const imageItems = messageContent.filter((item: any) => item.image);
					if (imageItems.length > 0) {
						images = imageItems.map(
							(item: any): ImageObject => ({
								type: "image_url",
								image_url: {
									url: item.image,
								},
							}),
						);
						content = "Generated image";
						finishReason = alibabaChoices[0]?.finish_reason || "stop";
						// DashScope image generation uses different usage format
						promptTokens = 0;
						completionTokens = 0;
						totalTokens = 0;
					} else {
						// Text content in DashScope format
						content =
							messageContent
								.filter((item: any) => item.text)
								.map((item: any) => item.text)
								.join("") || null;
						finishReason = alibabaChoices[0]?.finish_reason || null;
					}
				}
			} else if (json.choices) {
				// Alibaba chat completions use OpenAI format
				toolResults = json.choices?.[0]?.message?.tool_calls || null;
				content = json.choices?.[0]?.message?.content || null;
				reasoningContent =
					json.choices?.[0]?.message?.reasoning ||
					json.choices?.[0]?.message?.reasoning_content ||
					null;
				finishReason = json.choices?.[0]?.finish_reason || null;
				promptTokens = json.usage?.prompt_tokens || null;
				completionTokens = json.usage?.completion_tokens || null;
				reasoningTokens = json.usage?.reasoning_tokens || null;
				cachedTokens = json.usage?.prompt_tokens_details?.cached_tokens || null;
				totalTokens =
					json.usage?.total_tokens ||
					(promptTokens !== null && completionTokens !== null
						? promptTokens + completionTokens + (reasoningTokens || 0)
						: null);
				if (json.choices?.[0]?.message?.images) {
					images = json.choices[0].message.images;
				}
			}
			break;
		}
		default: // OpenAI format
			// Check if this is a Z.AI CogView image generation response
			// Format: { created: number, data: [{ url: "..." }] }
			if (usedProvider === "zai" && json.data && Array.isArray(json.data)) {
				const imageData = json.data;
				if (imageData.length > 0) {
					images = imageData.map(
						(item: any): ImageObject => ({
							type: "image_url",
							image_url: {
								url: item.url,
							},
						}),
					);
					content = "Generated image";
					finishReason = "stop";
					// CogView image generation doesn't return token usage
					promptTokens = 0;
					completionTokens = 0;
					totalTokens = 0;
				}
				break;
			}
			// Check if this is an OpenAI responses format (has output array instead of choices)
			if (json.output && Array.isArray(json.output)) {
				// OpenAI responses endpoint format
				const messageOutput = json.output.find(
					(item: any) => item.type === "message",
				);
				const reasoningOutput = json.output.find(
					(item: any) => item.type === "reasoning",
				);

				// Extract message content
				if (messageOutput?.content?.[0]?.text) {
					content = messageOutput.content[0].text;
				}

				// Extract reasoning content from summary
				if (reasoningOutput?.summary?.[0]?.text) {
					reasoningContent = reasoningOutput.summary[0].text;
				}

				// Extract tool calls (if any) from the output array and transform to OpenAI format
				const functionCalls = json.output.filter(
					(item: any) => item.type === "function_call",
				);
				if (functionCalls.length > 0) {
					toolResults = functionCalls.map((functionCall: any) => ({
						id: functionCall.call_id || functionCall.id,
						type: "function",
						function: {
							name: functionCall.name,
							arguments: functionCall.arguments,
						},
					}));
				} else {
					toolResults = null;
				}

				// Status mapping with tool call detection for responses API
				if (json.status === "completed") {
					// Check if there are tool calls in the response
					if (toolResults && toolResults.length > 0) {
						finishReason = "tool_calls";
					} else {
						finishReason = "stop";
					}
				} else {
					finishReason = json.status;
				}

				// Usage token extraction
				promptTokens = json.usage?.input_tokens || null;
				completionTokens = json.usage?.output_tokens || null;
				reasoningTokens =
					json.usage?.output_tokens_details?.reasoning_tokens || null;
				cachedTokens = json.usage?.input_tokens_details?.cached_tokens || null;
				totalTokens = json.usage?.total_tokens || null;

				// Count web_search_call items for pricing (each call is billed, not each citation)
				const webSearchCalls = json.output.filter(
					(item: any) => item.type === "web_search_call",
				);
				if (webSearchCalls.length > 0) {
					webSearchCount = webSearchCalls.length;
				}

				// Extract web search citations from OpenAI Responses API format
				// Citations come as annotations in the message content (for display, not pricing)
				if (messageOutput?.content) {
					for (const contentItem of messageOutput.content) {
						if (
							contentItem.annotations &&
							Array.isArray(contentItem.annotations)
						) {
							for (const annotation of contentItem.annotations) {
								if (annotation.type === "url_citation") {
									annotations.push({
										type: "url_citation",
										url_citation: {
											url: annotation.url || "",
											title: annotation.title,
											start_index: annotation.start_index,
											end_index: annotation.end_index,
										},
									});
								}
							}
						}
					}
				}
			} else {
				// Standard OpenAI chat completions format
				toolResults = json.choices?.[0]?.message?.tool_calls || null;
				content = json.choices?.[0]?.message?.content || null;
				// Extract reasoning content for reasoning-capable models
				// Check both reasoning and reasoning_content (GLM models use reasoning_content)
				reasoningContent =
					json.choices?.[0]?.message?.reasoning ||
					json.choices?.[0]?.message?.reasoning_content ||
					null;
				finishReason = json.choices?.[0]?.finish_reason || null;

				// ZAI-specific fix for incorrect finish_reason in tool response scenarios
				// Only for models that were failing tests: glm-4.5-airx and glm-4.5-flash
				if (
					usedProvider === "zai" &&
					finishReason === "tool_calls" &&
					messages.length > 0
				) {
					const lastMessage = messages[messages.length - 1];
					const modelName = json.model;

					// Only apply to specific failing models and only when last message was a tool result
					if (
						(modelName === "glm-4.5-airx" || modelName === "glm-4.5-flash") &&
						lastMessage?.role === "tool"
					) {
						// Check if the response actually contains new tool calls that should be prevented
						const hasNewToolCalls =
							json.choices?.[0]?.message?.tool_calls?.length > 0;
						if (hasNewToolCalls) {
							finishReason = "stop";
							// Also update JSON to match
							if (json.choices?.[0]) {
								json.choices[0].finish_reason = "stop";
								delete json.choices[0].message.tool_calls;
							}
						}
					}
				}

				// Standard OpenAI-style token parsing
				promptTokens = json.usage?.prompt_tokens || null;
				completionTokens = json.usage?.completion_tokens || null;
				reasoningTokens = json.usage?.reasoning_tokens || null;
				cachedTokens = json.usage?.prompt_tokens_details?.cached_tokens || null;
				totalTokens =
					json.usage?.total_tokens ||
					(promptTokens !== null && completionTokens !== null
						? promptTokens + completionTokens + (reasoningTokens || 0)
						: null);

				// Extract images from OpenAI-format response (including Gemini via gateway)
				if (json.choices?.[0]?.message?.images) {
					images = json.choices[0].message.images;
				}

				// Extract web search citations from OpenAI Chat Completions format
				// For search models, citations come in message.annotations
				// Count as 1 search per request if any citations are present (billed per request, not per citation)
				const messageAnnotations =
					json.choices?.[0]?.message?.annotations || [];
				let hasSearchCitations = false;
				for (const annotation of messageAnnotations) {
					if (annotation.type === "url_citation") {
						hasSearchCitations = true;
						annotations.push({
							type: "url_citation",
							url_citation: {
								url: annotation.url_citation?.url || annotation.url || "",
								title: annotation.url_citation?.title || annotation.title,
								start_index:
									annotation.url_citation?.start_index ??
									annotation.start_index,
								end_index:
									annotation.url_citation?.end_index ?? annotation.end_index,
							},
						});
					}
				}
				if (hasSearchCitations) {
					webSearchCount = 1; // Search models bill per request, not per citation
				}

				// For ZAI, extract web search info if present
				// ZAI includes web_search content in the response
				if (usedProvider === "zai") {
					const webSearchResults =
						json.choices?.[0]?.message?.web_search || null;
					if (webSearchResults && Array.isArray(webSearchResults)) {
						webSearchCount = webSearchResults.length;
						for (const result of webSearchResults) {
							annotations.push({
								type: "url_citation",
								url_citation: {
									url: result.link || result.url || "",
									title: result.title,
								},
							});
						}
					}
				}
			}
			break;
	}

	return {
		content,
		reasoningContent,
		finishReason,
		promptTokens,
		completionTokens,
		totalTokens,
		reasoningTokens,
		cachedTokens,
		toolResults,
		images,
		annotations: annotations.length > 0 ? annotations : null,
		webSearchCount: webSearchCount > 0 ? webSearchCount : null,
	};
}
